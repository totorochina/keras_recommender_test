{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于用户相似度做的协同推荐\n",
    "\n",
    "导入movielen数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reading ratings file\n",
    "# Ignore the timestamp column\n",
    "ratings = pd.read_csv('ratings.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "# Reading users file\n",
    "users = pd.read_csv('users.csv', sep='\\t', encoding='latin-1', usecols=['user_id', 'gender', 'zipcode', 'age_desc', 'occ_desc'])\n",
    "\n",
    "# Reading movies file\n",
    "movies = pd.read_csv('movies.csv', sep='\\t', encoding='latin-1', usecols=['movie_id', 'title', 'genres'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将\n",
    "users[['gender', 'age_desc', 'occ_desc']].head()\n",
    "\n",
    "  gender  age_desc              occ_desc\n",
    "0      F  Under 18          K-12 student\n",
    "1      M       56+         self-employed\n",
    "2      M     25-34             scientist\n",
    "3      M     45-49  executive/managerial\n",
    "4      M     25-34                writer\n",
    "\n",
    "提取成一个pd.Series，便于后续用TfidfVectorizer做数值化，\n",
    "\n",
    "0             [u'F', u'Under 18', u'K-12 student']\n",
    "1                 [u'M', u'56+', u'self-employed']\n",
    "2                   [u'M', u'25-34', u'scientist']\n",
    "3    [u'M', u'45-49', u'executive', u'managerial']\n",
    "4                      [u'M', u'25-34', u'writer']\n",
    "\n",
    "目前排除了zipcode，因为发现用了引入zipcode后的相关度矩阵会非常庞大，后续再研究一下原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.hstack([x[:-1], x[-1].split('/')]).tolist()\n",
    "\n",
    "# Should include zipcode??\n",
    "#uu = users[['gender', 'zipcode', 'age_desc', 'occ_desc']].values\n",
    "uu = users[['gender', 'age_desc', 'occ_desc']].values\n",
    "user_labels = pd.Series([f(x) for x in uu]).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6040, 196)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words='english')\n",
    "tfidf_user_matrix = tf.fit_transform(user_labels)\n",
    "print(tfidf_user_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于TfidfVectorizer已经将向量标准化为长度1，所以可以直接用两个向量的点积作为cosine的值来判断向量之间的相似程度；因此可以用性能更好的linear_kernel来处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.06452105\n",
      "  1.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         1.         0.         0.21496202 0.\n",
      "  0.         0.23125072 0.19158282 0.         0.18982012 0.23125072\n",
      "  0.         0.         0.2037839  0.         0.         0.\n",
      "  0.         0.17331325]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.35977795 0.         0.40328017 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.21496202 0.         1.         0.\n",
      "  0.         0.25139085 0.20826819 0.         0.20635197 0.25139085\n",
      "  0.         0.         0.22153189 0.         0.         0.\n",
      "  0.         0.18840749]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.32992226 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         1.         0.47314171 0.\n",
      "  0.39873218 0.3146739  0.         0.3146739  0.39171404 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.23125072 0.         0.25139085 0.\n",
      "  0.         1.         0.22404966 0.         0.22198825 1.\n",
      "  0.         0.         0.23831842 0.         0.         0.\n",
      "  0.         0.20268402]\n",
      " [0.         0.         0.19158282 0.         0.20826819 0.\n",
      "  0.         0.22404966 1.         0.         0.1839092  0.22404966\n",
      "  0.         0.         0.19743815 0.         0.         0.\n",
      "  0.         0.16791635]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         1.         0.47314171 0.\n",
      "  0.39873218 0.3146739  0.         0.3146739  0.39171404 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.18982012 0.         0.20635197 0.\n",
      "  0.47314171 0.22198825 0.1839092  0.47314171 1.         0.22198825\n",
      "  0.42785924 0.         0.19562158 0.         0.42032844 0.\n",
      "  0.         0.1663714 ]\n",
      " [0.         0.         0.23125072 0.         0.25139085 0.\n",
      "  0.         1.         0.22404966 0.         0.22198825 1.\n",
      "  0.         0.         0.23831842 0.         0.         0.\n",
      "  0.         0.20268402]\n",
      " [0.         0.         0.         0.35977795 0.         0.\n",
      "  0.39873218 0.         0.         0.39873218 0.42785924 0.\n",
      "  1.         0.         0.         0.         0.35422468 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.3146739  0.         0.         0.3146739  0.         0.\n",
      "  0.         1.         0.         1.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.2037839  0.40328017 0.22153189 0.\n",
      "  0.         0.23831842 0.19743815 0.         0.19562158 0.23831842\n",
      "  0.         0.         1.         0.         0.         0.\n",
      "  0.         0.17861022]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.3146739  0.         0.         0.3146739  0.         0.\n",
      "  0.         1.         0.         1.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.32992226\n",
      "  0.39171404 0.         0.         0.39171404 0.42032844 0.\n",
      "  0.35422468 0.         0.         0.         1.         0.\n",
      "  0.         0.        ]\n",
      " [0.06452105 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         1.\n",
      "  0.06452105 0.        ]\n",
      " [1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.06452105\n",
      "  1.         0.        ]\n",
      " [0.         0.         0.17331325 0.         0.18840749 0.\n",
      "  0.         0.20268402 0.16791635 0.         0.1663714  0.20268402\n",
      "  0.         0.         0.17861022 0.         0.         0.\n",
      "  0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_sim = linear_kernel(tfidf_user_matrix, tfidf_user_matrix)\n",
    "print(cosine_sim[:20, :20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备索引，便于后续查找数据，\n",
    "\n",
    "titles：根据索引得到电影名称\n",
    "indices：根据电影名称得到索引\n",
    "indices_movid：根据电影id得到电影的索引（电影id与索引的对应并不完全对齐）\n",
    "indices_userid：根据用户id得到用户的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a 1-dimensional array with movie titles\n",
    "titles = movies['title']\n",
    "indices = pd.Series(movies.index, index=movies['title'])\n",
    "\n",
    "indices_userid = pd.Series(users.index, index=users['user_id'])\n",
    "indices_movid = pd.Series(movies.index, index=movies['movie_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据用户id，返回最近似的用户，打分最高的电影。\n",
    "对所有高分电影算近似用户的平均值，返回前n个相似用户平均分最高的电影。\n",
    "\n",
    "简单验证user_id为1的用户的推荐结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[318, 2167, 1073, 899, 1408, 2628, 364, 3635, 1197, 2761]\n"
     ]
    }
   ],
   "source": [
    "def get_first_n(l, n=3):\n",
    "    \"\"\"Get the first n results from l\n",
    "    l is like,\n",
    "    l = [(1, 1.0), (2, 1.0), (3, 0.8), (4, 0.75), (5, 0.6), (6, 0.4), (7, 1.0), (8, 1.0)]\n",
    "    \n",
    "    randomnize and return n of them if candidates more than n\n",
    "    otherwise just sort and return first n of them\n",
    "    \"\"\"\n",
    "    l = sorted(l, key=lambda x:x[1], reverse=True)\n",
    "    top = l[0][-1]\n",
    "    \n",
    "    result = []\n",
    "    for x in l:\n",
    "        if x[1] == top:\n",
    "            result.append(x)\n",
    "    \n",
    "    if len(result) >= n:\n",
    "        random.shuffle(result)\n",
    "        return result[:n]\n",
    "    \n",
    "    else:\n",
    "        return l[:n]\n",
    "\n",
    "# Function that get movie recommendations based on the cosine similarity score of movie genres\n",
    "def genre_recommend_by_user_similarity(user_id, n = 10, k = 10):\n",
    "    \"\"\"For each user_id,\n",
    "    1. find the top n most similiar users\n",
    "    2. find their highest rated movies_id\n",
    "    3. shuffle and return the k of them\n",
    "    \"\"\"\n",
    "    \n",
    "    idx = indices_userid[user_id]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    users_id_list = [np.asscalar(users[users.index == x[0]].user_id) for x in get_first_n(sim_scores, n=n)]\n",
    "    \n",
    "    #print(users_id_list)\n",
    "    \n",
    "    y = None\n",
    "    \n",
    "    for user_id in users_id_list:\n",
    "        highest_rate_movie = \\\n",
    "            ratings[(ratings.user_id == user_id) & \\\n",
    "                    (ratings.rating == np.max(ratings[ratings.user_id == user_id].rating))][['movie_id']]\n",
    "        #print(user_id, highest_rate_movie.movie_id.values)\n",
    "        x = ratings[(ratings.user_id == user_id) & (ratings.movie_id.isin(highest_rate_movie.movie_id.values))][['user_id', 'movie_id', 'rating']]\n",
    "        y = pd.concat([x, y])\n",
    "        \n",
    "    # Average ratings for each high rated movies\n",
    "    z = y.groupby('movie_id').rating.sum()/y.groupby('movie_id').rating.count()\n",
    "    \n",
    "    result = get_first_n(list(zip(z.index, z.values)), n=k)\n",
    "    \n",
    "    # Return the movie id lists\n",
    "    result = [i[0] for i in result]\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(genre_recommend_by_user_similarity(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据集以默认1：3的比例以随机的方式分为训练和验证的部分，后续用验证集来验证推荐效果。 具体见， https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train & test data\n",
    "ratings_train, ratings_test = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过推荐列表的命中率验证效果，使用验证集来验证推荐结果，训练集用来生成推荐列表。 由于movielen里除了有评价过的电影还有相应的评分，为了贴合实际，认为， 只有命中且评分高于该用户的p80的评分（有些人习惯打高分，其他人反之），才算命中\n",
    "\n",
    "会执行很久，但可以像下列例子那样仅验证头100个user的推荐的命中率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19.0, 990.0)\n",
      "hit ratio percentage: 1.9191919192%\n"
     ]
    }
   ],
   "source": [
    "def hit_ratio_benchmark(ratings_train, ratings_test, rated_movie_limit=10):\n",
    "    \"\"\"\n",
    "    for each user_id\n",
    "    1. get recommend list using ratings_train rated movies\n",
    "    2. use ratings_test rated movies to validate hit ratio\n",
    "    it is considered hit when,\n",
    "    1. user rated this movie\n",
    "    2. the rate is >= this user's p80 rate in ratings_train\n",
    "    \"\"\"\n",
    "    hit, count = (0., 0.)\n",
    "    \n",
    "    #for user_id in np.sort(ratings_test['user_id'].unique()):\n",
    "    for user_id in np.arange(1, 100):\n",
    "        recommend_list = genre_recommend_by_user_similarity(user_id)\n",
    "        \n",
    "        for item in recommend_list:\n",
    "            count += 1\n",
    "            x = ratings_test[ratings_test.user_id == user_id][['movie_id', 'rating']]\n",
    "            if x[x.movie_id == item].empty:\n",
    "                continue\n",
    "            elif x[x.movie_id == item].rating.values < np.percentile(x.rating.values, 80):\n",
    "                continue\n",
    "            else:\n",
    "                hit += 1\n",
    "    \n",
    "    print(hit, count)\n",
    "    #hit_ratio = hit / ratings_test.movie_id.count() * 1.0\n",
    "    hit_ratio = hit / count\n",
    "    return hit_ratio\n",
    "        \n",
    "        \n",
    "hit_ratio = hit_ratio_benchmark(ratings_train, ratings_test)\n",
    "hit_ratio *= 100\n",
    "print('hit ratio percentage: %.10f%%' % hit_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDCG原理见， http://sofasofa.io/forum_main_post.php?postid=1002561"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> dcg_at_k(r, 1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 1, method=1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 2)\n",
    "    5.0\n",
    "    >>> dcg_at_k(r, 2, method=1)\n",
    "    4.2618595071429155\n",
    "    >>> dcg_at_k(r, 10)\n",
    "    9.6051177391888114\n",
    "    >>> dcg_at_k(r, 11)\n",
    "    9.6051177391888114\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> ndcg_at_k(r, 1)\n",
    "    1.0\n",
    "    >>> r = [2, 1, 2, 0]\n",
    "    >>> ndcg_at_k(r, 4)\n",
    "    0.9203032077642922\n",
    "    >>> ndcg_at_k(r, 4, method=1)\n",
    "    0.96519546960144276\n",
    "    >>> ndcg_at_k([0], 1)\n",
    "    0.0\n",
    "    >>> ndcg_at_k([1], 2)\n",
    "    1.0\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对所有验证集的用户的推荐列表，计算ndcg并计算他们的平均值，作为该推荐算法的ndcg分数\n",
    "由于ndcg计算的是推荐列表的顺序的精确度，因此如果验证集中该用户没有给推荐的电影打分，就认为是打了0分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14221969665559114\n"
     ]
    }
   ],
   "source": [
    "def ndcg_benchmark(ratings_train, ratings_test, rated_movie_limit=10):\n",
    "    \"\"\"\n",
    "    for each user_id\n",
    "    1. get recommend list using ratings_train rated movies\n",
    "    2. use ratings_test rated movies to validate ndcg value\n",
    "    if it is not rated, make it zero\n",
    "    return average ndcg_score for all ratings_test users\n",
    "    \"\"\"\n",
    "    \n",
    "    ndcg_score, count = (0, 0)\n",
    "    #for user_id in np.sort(ratings_test['user_id'].unique()):\n",
    "    for user_id in np.arange(1, 100):\n",
    "        r = []\n",
    "        #print('user_id %s' % user_id)\n",
    "        \n",
    "        recommend_list = genre_recommend_by_user_similarity(user_id)\n",
    "        #print(user_id, recommend_list)\n",
    "        \n",
    "        for item in recommend_list:\n",
    "            x = ratings_test[ratings_test.user_id == user_id][['movie_id', 'rating']]\n",
    "            if x[x.movie_id == item].empty:\n",
    "                r.append(0)\n",
    "            else:\n",
    "                r.append(\\\n",
    "                    np.asscalar(ratings_test[(ratings_test.user_id == user_id) & \\\n",
    "                        (ratings_test.movie_id == item)].rating.values))\n",
    "    \n",
    "        ndcg_score += ndcg_at_k(r, len(r))\n",
    "        count += 1.0\n",
    "\n",
    "    ndcg_score /= count\n",
    "    return ndcg_score\n",
    "\n",
    "print(ndcg_benchmark(ratings_train, ratings_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到hit ratio和NDCG的值都偏低。原因包括该验证集并不是由新的推荐算法的产生的，实际生产中更多会通过A/B方式做验证。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py2] *",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
